{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Batch Transform custom TensorFlow inference.py (CSV & TFRecord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "We will train a simple classifier on the iris dataset, training locally from where this notebook is being run. We will then write a custom `inference.py` script for `CSV` and `TFRecord` data that will be used when hosting our model in a Batch Transform Job.\n",
    "\n",
    "Consider the following model definition for IRIS classification. This mode uses the ``tensorflow.estimator.DNNClassifier`` which is a pre-defined estimator module for its model definition.\n",
    "\n",
    "* [Prequisites](#prequisites)\n",
    "* [Training the Network Locally](#training-the-network-locally)\n",
    "* [Set the model up for hosting ](#hosting)\n",
    "* [Write a custom inference.py script](#inference-script)\n",
    "* [ Create Batch Transform Job](#transform)\n",
    "\n",
    "\n",
    "## Prequisites  <a class=\"anchor\" id=\"prequisites\"></a>\n",
    "### Packages and Permissions\n",
    "\n",
    "Here we set up the specific TensorFlow version we will use to Train and Host our model. The SageMaker SDK will use S3 defualt buckets when needed. If the ``get_execution_role``  does not return a role with the appropriate permissions, you'll need to specify an IAM role arn that does. Please make use of an execustion role with `SageMakerFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.1\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from datetime import datetime \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll use a very simple network architecture, with three densely-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_mlp(metrics):\n",
    "    ### Setup loss and output node activation\n",
    "    output_activation = \"softmax\"\n",
    "    loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    input = Input(shape=(4,), name=\"input\")\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer1\",\n",
    "    )(input)\n",
    "\n",
    "    x = Dense(\n",
    "        units=20,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        activation=\"relu\",\n",
    "        name=\"dense_layer2\",\n",
    "    )(x)\n",
    "\n",
    "    x = Dense(\n",
    "        units=10,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        name=\"dense_layer3\",\n",
    "    )(x)\n",
    "\n",
    "    output = Dense(units=3, activation=output_activation)(x)\n",
    "\n",
    "    ### Compile the model\n",
    "    model = tf.keras.Model(input, output)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the pre-processed iris training and test data stored in the `sagemaker-sample-files` public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download iris test and train data sets from S3\n",
    "SOURCE_DATA_BUCKET = \"sagemaker-sample-files\"\n",
    "SOURCE_DATA_PREFIX = \"datasets/tabular/iris\"\n",
    "sm_session.download_data(\".\", bucket=SOURCE_DATA_BUCKET, key_prefix=SOURCE_DATA_PREFIX)\n",
    "\n",
    "# Load the training and test data from .csv to a Pandas data frame.\n",
    "train_df = pd.read_csv(\n",
    "    \"iris_train.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    \"iris_test.csv\",\n",
    "    header=0,\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "\n",
    "# Pop the record labels into N x 1 Numpy arrays\n",
    "train_labels = np.array(train_df.pop(\"class\"))\n",
    "test_labels = np.array(test_df.pop(\"class\"))\n",
    "\n",
    "# Save the remaining features as Numpy arrays\n",
    "train_np = np.array(train_df)\n",
    "test_np = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network Locally <a class=\"anchor\" id=\"training-the-network-locally\"></a> \n",
    "Here, we train the network using the Tensorflow .fit method, just like if we were using our local computers. This should only take a few seconds because the model is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.0429 - accuracy: 0.6167 - binary_accuracy: 0.3500 - val_loss: 1.0696 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.0160 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 1.0489 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9901 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 1.0298 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9672 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 1.0115 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9463 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 0.9955 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9264 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 0.9793 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9046 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 0.9634 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8831 - accuracy: 0.7000 - binary_accuracy: 0.3500 - val_loss: 0.9481 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8620 - accuracy: 0.7083 - binary_accuracy: 0.3500 - val_loss: 0.9323 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8414 - accuracy: 0.7083 - binary_accuracy: 0.3444 - val_loss: 0.9152 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2333\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.8196 - accuracy: 0.7250 - binary_accuracy: 0.3167 - val_loss: 0.8958 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2000\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7981 - accuracy: 0.7250 - binary_accuracy: 0.2722 - val_loss: 0.8764 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7761 - accuracy: 0.7333 - binary_accuracy: 0.2500 - val_loss: 0.8569 - val_accuracy: 0.5333 - val_binary_accuracy: 0.1778\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7538 - accuracy: 0.7333 - binary_accuracy: 0.2444 - val_loss: 0.8356 - val_accuracy: 0.5333 - val_binary_accuracy: 0.2000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7317 - accuracy: 0.7500 - binary_accuracy: 0.2444 - val_loss: 0.8143 - val_accuracy: 0.5667 - val_binary_accuracy: 0.2000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.7099 - accuracy: 0.7500 - binary_accuracy: 0.2444 - val_loss: 0.7933 - val_accuracy: 0.6000 - val_binary_accuracy: 0.2000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.7833 - binary_accuracy: 0.2444 - val_loss: 0.7730 - val_accuracy: 0.6333 - val_binary_accuracy: 0.2000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6658 - accuracy: 0.7917 - binary_accuracy: 0.2444 - val_loss: 0.7521 - val_accuracy: 0.6667 - val_binary_accuracy: 0.2000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6443 - accuracy: 0.8083 - binary_accuracy: 0.2472 - val_loss: 0.7311 - val_accuracy: 0.6667 - val_binary_accuracy: 0.2000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6229 - accuracy: 0.8167 - binary_accuracy: 0.2472 - val_loss: 0.7104 - val_accuracy: 0.7000 - val_binary_accuracy: 0.2000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6018 - accuracy: 0.8250 - binary_accuracy: 0.2472 - val_loss: 0.6906 - val_accuracy: 0.7000 - val_binary_accuracy: 0.2000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5810 - accuracy: 0.8333 - binary_accuracy: 0.2528 - val_loss: 0.6711 - val_accuracy: 0.7000 - val_binary_accuracy: 0.2000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5612 - accuracy: 0.8333 - binary_accuracy: 0.2556 - val_loss: 0.6516 - val_accuracy: 0.7000 - val_binary_accuracy: 0.2000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5416 - accuracy: 0.8333 - binary_accuracy: 0.2500 - val_loss: 0.6285 - val_accuracy: 0.7667 - val_binary_accuracy: 0.2000\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5239 - accuracy: 0.8583 - binary_accuracy: 0.2472 - val_loss: 0.6068 - val_accuracy: 0.8000 - val_binary_accuracy: 0.2000\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5049 - accuracy: 0.8750 - binary_accuracy: 0.2472 - val_loss: 0.5890 - val_accuracy: 0.8000 - val_binary_accuracy: 0.2000\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4879 - accuracy: 0.8750 - binary_accuracy: 0.2472 - val_loss: 0.5702 - val_accuracy: 0.8000 - val_binary_accuracy: 0.2000\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4712 - accuracy: 0.8750 - binary_accuracy: 0.2500 - val_loss: 0.5555 - val_accuracy: 0.8000 - val_binary_accuracy: 0.2000\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4556 - accuracy: 0.8750 - binary_accuracy: 0.2556 - val_loss: 0.5356 - val_accuracy: 0.8000 - val_binary_accuracy: 0.2000\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.9000 - binary_accuracy: 0.2639 - val_loss: 0.5183 - val_accuracy: 0.8667 - val_binary_accuracy: 0.2333\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4257 - accuracy: 0.9000 - binary_accuracy: 0.2778 - val_loss: 0.5030 - val_accuracy: 0.8667 - val_binary_accuracy: 0.2556\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4125 - accuracy: 0.9083 - binary_accuracy: 0.2889 - val_loss: 0.4866 - val_accuracy: 0.9000 - val_binary_accuracy: 0.2778\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.9167 - binary_accuracy: 0.2944 - val_loss: 0.4716 - val_accuracy: 0.9000 - val_binary_accuracy: 0.2778\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3872 - accuracy: 0.9167 - binary_accuracy: 0.3028 - val_loss: 0.4526 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3111\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3755 - accuracy: 0.9583 - binary_accuracy: 0.3167 - val_loss: 0.4392 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3111\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3642 - accuracy: 0.9500 - binary_accuracy: 0.3194 - val_loss: 0.4289 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3222\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3535 - accuracy: 0.9333 - binary_accuracy: 0.3194 - val_loss: 0.4181 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3222\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3437 - accuracy: 0.9417 - binary_accuracy: 0.3194 - val_loss: 0.4038 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3222\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.9583 - binary_accuracy: 0.3306 - val_loss: 0.3885 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3333\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.3265 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3804 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3333\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3688 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3222\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3096 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3611 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3333\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3029 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3475 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3222\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2950 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3392 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3222\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2872 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3379 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3333\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2800 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3247 - val_accuracy: 0.9333 - val_binary_accuracy: 0.3222\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2710 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.3106 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3333\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2631 - accuracy: 0.9583 - binary_accuracy: 0.3333 - val_loss: 0.2979 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3333\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2562 - accuracy: 0.9667 - binary_accuracy: 0.3306 - val_loss: 0.2874 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3333\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2495 - accuracy: 0.9667 - binary_accuracy: 0.3333 - val_loss: 0.2782 - val_accuracy: 0.9667 - val_binary_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", mode=\"auto\", restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Instantiate classifier\n",
    "classifier = iris_mlp(metrics=[\"accuracy\", \"binary_accuracy\"])\n",
    "\n",
    "# Fit classifier\n",
    "history = classifier.fit(\n",
    "    x=train_np,\n",
    "    y=train_labels,\n",
    "    validation_data=(test_np, test_labels),\n",
    "    callbacks=[EARLY_STOPPING],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model up for hosting  <a class=\"anchor\" id=\"hosting\"></a>\n",
    "\n",
    "### Export the model from TensorFlow\n",
    "SageMaker TensorFlow Serving container expects the model artifacts in the following format:\n",
    "\n",
    "```\n",
    "1\n",
    "├── keras_metadata.pb\n",
    "├── saved_model.pb\n",
    "└── variables\n",
    "    ├── variables.data-00000-of-00001\n",
    "    └── variables.index\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    }
   ],
   "source": [
    "classifier.save(\"1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new sagemaker session and upload the model on to the default S3 bucket. We can use the ``sagemaker.Session.upload_data`` method to do this. We need the location of where we exported the model from TensorFlow and where in our default bucket we want to store the model(``/model``). The default S3 bucket can be found using the ``sagemaker.Session.default_bucket`` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-171503325295/model/model.tar.gz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=\"model\")\n",
    "s3_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model input tensor shape\n",
    "We can use the the `saved_model_cli` to view the model's input tensors which will help us in building our custom inference.py script.\n",
    "\n",
    "As we can see our model expects input in the shape of (-1, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 21:03:04.689491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 4)\n",
      "        name: serving_default_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 3)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir {\"1\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example Batch Data\n",
    "Below we view the sample CSV data that will be used as input to our Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9,3.0,4.2,1.5\n",
      "6.9,3.1,5.4,2.1\n",
      "5.1,3.3,1.7,0.5\n",
      "6.0,3.4,4.5,1.6\n",
      "5.5,2.5,4.0,1.3\n",
      "6.2,2.9,4.3,1.3\n",
      "5.5,4.2,1.4,0.2\n",
      "6.3,2.8,5.1,1.5\n",
      "5.6,3.0,4.1,1.3\n",
      "6.7,2.5,5.8,1.8\n"
     ]
    }
   ],
   "source": [
    "!head Data/batch-iris-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-171503325295/datasets/batch-iris-data.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_csv_data = \"s3://{}/datasets/batch-iris-data.csv\".format(bucket_name)\n",
    "s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: Data/batch-iris-data.csv to s3://sagemaker-us-east-1-171503325295/datasets/batch-iris-data.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp Data/batch-iris-data.csv $s3_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Example Batch Data\n",
    "We will use the CSV data to generate TFRecord data that we will also use as inference.\n",
    "I.e the CSV data and TFRecord identical they are just of different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import tensorflow as tf\n",
    "csv = pandas.read_csv(\"Data/batch-iris-data.csv\", header=None).values\n",
    "\n",
    "with tf.io.TFRecordWriter(\"Data/batch-iris-data.tfrecords\") as writer:\n",
    "    for row in csv:\n",
    "        features = row[:]\n",
    "        example = tf.train.Example()\n",
    "        example.features.feature[\"features\"].float_list.value.extend(features)\n",
    "      \n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"features\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 5.900000095367432\n",
      "        value: 3.0\n",
      "        value: 4.199999809265137\n",
      "        value: 1.5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(\"Data/batch-iris-data.tfrecords\")\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-171503325295/datasets/batch-iris-data.tfrecords'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_tf_record_data = \"s3://{}/datasets/batch-iris-data.tfrecords\".format(bucket_name)\n",
    "s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload TFRecord input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: Data/batch-iris-data.tfrecords to s3://sagemaker-us-east-1-171503325295/datasets/batch-iris-data.tfrecords\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp Data/batch-iris-data.tfrecords $s3_tf_record_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a custom inference.py script <a class=\"anchor\" id=\"inference-script\"></a>\n",
    "\n",
    "Our model accepts a tensor of (-1, 4). Hence, we will create an input handler for each file type (text/csv, application/x-tfrecord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import json\n",
    "import os \n",
    "os.system(\"pip install numpy tensorflow crcmod\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "\n",
    "import crcmod\n",
    "\n",
    "def _masked_crc32c(value):\n",
    "    crc = crcmod.predefined.mkPredefinedCrcFun(\"crc-32c\")(value)\n",
    "    return (((crc >> 15) | (crc << 17)) + 0xA282EAD8) & 0xFFFFFFFF\n",
    "\n",
    "def read_tfrecords(tfrecords):\n",
    "    import io\n",
    "    import struct\n",
    "    tfrecords_bytes = io.BytesIO(tfrecords)\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    while True:\n",
    "        length_header = 12\n",
    "        buf = tfrecords_bytes.read(length_header)\n",
    "        if not buf:\n",
    "            # reached end of tfrecord buffer, return examples\n",
    "            return examples\n",
    "\n",
    "        if len(buf) != length_header:\n",
    "            raise ValueError(\"TFrecord is fewer than %d bytes\" % length_header)\n",
    "        length, length_mask = struct.unpack(\"<QI\", buf)\n",
    "        length_mask_actual = _masked_crc32c(buf[:8])\n",
    "        if length_mask_actual != length_mask:\n",
    "            raise ValueError(\"TFRecord does not contain a valid length mask\")\n",
    "\n",
    "        length_data = length + 4\n",
    "        buf = tfrecords_bytes.read(length_data)\n",
    "        if len(buf) != length_data:\n",
    "            raise ValueError(\"TFRecord data payload has fewer bytes than specified in header\")\n",
    "        data, data_mask_expected = struct.unpack(\"<%dsI\" % length, buf)\n",
    "        data_mask_actual = _masked_crc32c(data)\n",
    "        if data_mask_actual != data_mask_expected:\n",
    "            raise ValueError(\"TFRecord has an invalid data crc32c\")\n",
    "\n",
    "        # Deserialize the tf.Example proto\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(data)\n",
    "        example_features = MessageToDict(example)['features']['feature']['features']['floatList']['value']\n",
    "        # Extract a feature map from the example object\n",
    "        examples.append(example_features)\n",
    "        \n",
    "    return examples\n",
    "\n",
    "def read_csv(csv):\n",
    "      return np.array([[float(j) for j in i.split(',')] for i in csv.splitlines()])\n",
    "\n",
    "\n",
    "def input_handler(data, context):\n",
    "    \"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\n",
    "    Args:\n",
    "        data (obj): the request data stream\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (dict): a JSON-serializable dict that contains request body and headers\n",
    "    \"\"\"\n",
    "    \n",
    "    if context.request_content_type == 'text/csv':\n",
    "\n",
    "        payload = data.read().decode(\"utf-8\")\n",
    "        inputs = read_csv(payload)\n",
    "      \n",
    "        input = {\n",
    "            'inputs': inputs.tolist()\n",
    "            }\n",
    "        \n",
    "     \n",
    "        return json.dumps(input)\n",
    "    \n",
    "    if context.request_content_type == \"application/x-tfrecord\":\n",
    "    \n",
    "        payload = data.read()\n",
    "        examples = read_tfrecords(payload)\n",
    "        \n",
    "        input = {\n",
    "            'inputs': examples\n",
    "            }\n",
    "        \n",
    "   \n",
    "        return json.dumps(input)\n",
    "        \n",
    "\n",
    "    raise ValueError('{{\"error\": \"unsupported content type {}\"}}'.format(\n",
    "            context.request_content_type or \"unknown\"))\n",
    "\n",
    "    \n",
    "def output_handler(data, context):\n",
    "    \"\"\"Post-process TensorFlow Serving output before it is returned to the client.\n",
    "    Args:\n",
    "        data (obj): the TensorFlow serving response\n",
    "        context (Context): an object containing request and configuration details\n",
    "    Returns:\n",
    "        (bytes, string): data to return to client, response content type\n",
    "    \"\"\"\n",
    "\n",
    "    if data.status_code != 200:\n",
    "        raise ValueError(data.content.decode('utf-8'))\n",
    "\n",
    "    response_content_type = context.accept_header\n",
    "    prediction = data.content\n",
    "    return prediction, response_content_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm the input_handler for TFRecord and CSV return the same output\n",
    "Due to the fact that our TFRecord and CSV input data is the same, this means the input_handler should have identical output for each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV and TFRecord output match\n"
     ]
    }
   ],
   "source": [
    "import inference\n",
    "class Context:\n",
    "    def __init__(self, request_content_type):\n",
    "        self.request_content_type = request_content_type\n",
    "\n",
    "  \n",
    "tfrecord_bytes = open(\"Data/batch-iris-data.tfrecords\", \"rb\")\n",
    "tfrecord_input = inference.input_handler(tfrecord_bytes, Context(\"application/x-tfrecord\"))\n",
    "\n",
    "csv_file_bytes = open(\"Data/batch-iris-data.csv\", \"rb\")\n",
    "csv_input = inference.input_handler(csv_file_bytes, Context(\"text/csv\"))\n",
    "\n",
    "assert csv_input == tfrecord_input, \"CSV and TFRecord output do not match!\"\n",
    "print(\"CSV and TFRecord output match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Batch Transform Job <a class=\"anchor\" id=\"transform\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker TensorFlow Model\n",
    "First we need to create a `TensorFlowModel` that points to our S3 model tar ball in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "tensorflow_serving_model_batch = TensorFlowModel(\n",
    "    model_data=f\"s3://{bucket_name}/model/model.tar.gz\",\n",
    "    entry_point='inference.py',\n",
    "    role=role,\n",
    "    framework_version=\"2.3.1\",\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Input Transform Job \n",
    "Create the Transform Job by specifying the CSV input data S3 location and `content_type` as `text/csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.now().strftime('%Y-%m-%d-%H-%m-%S-%F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type=\"Line\"\n",
    "batch_strategy=\"MultiRecord\"\n",
    "\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "  \n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_csv_data, content_type=\"text/csv\", split_type=split_type,  wait=False,   job_name=\"tensorflow-inference-TFRecord-{}\".format(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Input Transform Job \n",
    "### CSV Input Transform Job\n",
    "Create the Transform Job by specifying the TFRecord input data S3 location and `content_type` as `application/x-tfrecord`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path_batch = \"s3://{}/output/batch_iris/\".format(bucket_name)\n",
    "\n",
    "output_data_path = output_data_path_batch\n",
    "batch_instance_count = 1\n",
    "batch_instance_type = \"ml.m5.4xlarge\"\n",
    "concurrency = 5\n",
    "max_payload_in_mb = 1\n",
    "split_type=\"TFRecord\"\n",
    "batch_strategy=\"MultiRecord\"\n",
    "\n",
    "transformer = tensorflow_serving_model_batch.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    strategy=batch_strategy,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=s3_tf_record_data, content_type=\"application/x-tfrecord\", split_type=split_type,  wait=False, job_name=\"tensorflow-inference-CSV-{}\".format(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Kindly navigate to the SageMaker Console to monitor the Job status.*"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
